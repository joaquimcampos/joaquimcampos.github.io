<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Deep Spline Neural Networks</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Joaquim Campos</strong></a>
									<ul class="icons">
										<!-- ?subject=subject -->
										<li><a href="mailto:joaquimcampos@duck.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
										<li><a href="https://scholar.google.com/citations?user=GT-VCroAAAAJ" target="_blank" class="icon fa-brands fa-google-scholar"><span class="label">Google Scholar</span></a></li>
										<li><a href="https://www.linkedin.com/in/joaquim-campos" target="_blank" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section id="project">
									<header class="main">
										<h1>Deep Spline Neural Networks <br>
										<span>@BIG.EPFL</span>
										</h1>
									</header>
									
									<div class="flex-box project-banner">
										<a id="deepsplines-img" class="flex-image" href="https://ieeexplore.ieee.org/document/9655475" target="_blank">
											<img src="images/deepsplines2.png" alt="Deepsplines">
										</a>
									</div>

									<h2>Introduction: Supervised Learning</h2>

									<p>
										<i>Note: It might be useful to read <a href="htv.html" target="_blank">this project's introduction</a> too</i>.
									</p>
									<p>
										Suppose we have a dataset \({\mathrm D}\) that consists of sequence of \(M\) images
										of cats and dogs, together with their respective labels.
										Mathematically, we write this statement as \({\mathrm D} = \lbrace{\mathbf x}_m, y_m\rbrace _{m=1}^M\),
										where \(y_m = 0\) if \({\mathbf x}_m\) represents a cat, and \(y_m = 1\) otherwise,
										as depicted below.
									</p>
									<div class="flex-box body-box">
										<div id="cat-dog-row">
											<img src="images/cat.png" alt="Cat">
											<img src="images/dog.png" alt="Dog">
										</div>
									</div>
									<p>
										In supervised learning, our goal is to find a model \(f\) that makes few classification mistakes for the images in our dataset <a href="#footnotes" class="tooltip tooltip-narrow bottom" data-tooltip='In technical terms, we say that our goal is to find a model that "fits the data".'>1</a>
										as well as new images that are not found in the training data
										<a href="#footnotes" class="tooltip bottom" data-tooltip='In technical terms, we say that we are looking for a model that "generalizes well to new inputs". The problem of
										having a model that fits the data but is not able to generalize is called overfitting.'>2</a>
										<a href="#footnotes" class="tooltip tooltip-narrow bottom" data-tooltip='I am restricting myself here to classification for didactic purposes.'>3</a>.
									</p>

									<h4>Mathematical Details</h4>
									<p>
										In the cat-or-dog example, the output of our model is a two-element vector where each value represents the "confidence" in each label, and the predictions \({\hat y}_m\) are assigned according to the label with the highest confidence; that is,
										$$
										\begin{align*}
										f({\mathbf x}_m) & = (a_{m,0}, a_{m,1}) \\[5pt]
										{\hat y}_m & = 
										\begin{cases}
										0 & \text{if} \ \ a_{m,0} \geq a_{m,1} \\
										1 & \text{if} \ \ a_{m,0} < a_{m,1}
										\end{cases}
										\end{align*} 
										$$
									</p>

									<h3>Neural Networks</h3>

									<p>
										Nowadays, neural networks are the most standard way of solving supervised learning
										problems. Neural nets are sequences of simple linear transformations and pointwise
										nonlinearities called activation functions
										<a href="#footnotes" class="tooltip top" data-tooltip='A pointwise operation is one for which each output element depends on a single input element.'>4</a>.
										<!-- <i>(Skippable mathematical details)</i> <br>
										In mathematical terms, this is written as
										\[
											{\mathbf f}_{\boldsymbol{\Theta }}({\mathbf x}):
											{\mathbf W}_L \circ \cdots \circ {\boldsymbol{\sigma }}_\ell \circ {\mathbf W}_\ell \circ \dots \circ {\boldsymbol{\sigma }}_1 \circ {\mathbf W}_1 ({\mathbf x}),
										\]
										where \(L\) is the depth of the neural net and \({\boldsymbol \Theta}\) is a list of all the parameters that are adjusted during the learning process. <br>
										<i>(End of mathematics)</i> -->
										The most popular activation function is the
										<a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>,
										which is a continuous and piecewise-linear (CPWL) function,
										meaning that its graph is composed of
										straight-line segments (like this &#x1F4C9;).
										Interestingly, networks with CPWL activations are CPWL functions themselves.
									</p>

									<hr class="major" />

									<h2>The Projects, Simply Explained</h2>

									<h3>A Bit of Theory</h3>

									<p>
										In order to learn the activation functions of a neural network, we need a way
										to turn them into learnable modules that depend on discrete
										parameters, as is the case with the linear transformation
										<a href="#footnotes" class="tooltip tooltip-narrow bottom" data-tooltip='Otherwise, the problem is ill-posed.'>5</a>.
										To do this, we have to introduce some constraints.
									</p>
									<p>
										Since networks with CPWL activations seem to work very well, it would be
										great if we could mantain this behaviour. Well, it turns out that we can modify our problem statement slightly by adding a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> term called "second-order total variation",
										and have guarantees that there are optimal solutions to the problem (<i>i.e.</i>, optimal models) that actually have
										CPWL activation functions. In that case, we have a theoretical justification to restrict our search
										to such kinds of models.
										(For example, if you are looking for a very fast bike in a big bike shop, and someone tells you
										that the last Tour de France was won with a bike of this particular brand,
										then it's smarter to just try out bikes from that specific brand because
										you know that at least one of them will do the job well.)
									</p>

									<h3>The First Project: Deep Spline Networks</h3>

									<p>
										<i>
										(There are a few important steps in getting from the theory to the actual
										algorithm that we have ommited here for the sake of simplicity.
										If you'd like to know more details, you can read
										<a href="https://ieeexplore.ieee.org/document/9264754">the corresponding article</a>.)
										</i>
									</p>
									<p>
										In order to have learnable activation functions, we construct them using "lego pieces" (basis functions)
										called \(\beta ^1\)-splines, which are placed at equally-spaced locations.
										<a href="#deepsplines-img">In the image at the top</a>, you can see how several lego pieces work together to form a parametrized activation function
										<a href="#footnotes" class="tooltip tooltip-narrow top" data-tooltip='There are some extra boundary functions to make the location of the
										knots bounded to a region of interest.'>6</a>.
										During training, we learn the heights (coefficients) of the lego pieces together
										with the other parameters in the network.
										The models that can be found using our method are called "deep spline networks";
										they include all models with ReLU-like activations (ReLU, LeakyReLU, PReLU, etc.).
									</p>
									<p>
										Apart from being theoretically supported, our method has two practical key strengths:
										First, the output of the activatons at a specific location depends on (at most)
										two lego pieces because the lego pieces are localized, meaning
										that they are zero outside a small range.
										<a href="#footnotes" class="tooltip top" data-tooltip='This is not the case with ReLU basis functions because they are not localized (they are non-zero at any point to the right of the knot). Therefore, we say they are poorly-conditioned.'>7</a>.
										This makes <b>our method is stable (well-conditioned), and fast to run.</b>
										Second, <b>we can control the degree of simplicity of
										our activations&mdash;as measured by the number of knots/linear regions&mdash;by
										changing the strength of the regularization applied </b>
										<a href="#footnotes" class="tooltip bottom" data-tooltip='In technical terms, our method is able to enforce activation functions with sparse second-derivatives (in the weak sense).'>8</a>.
										Therefore, our approach compatible with the Occam's razor principle:
										as long as it can do the job, the simpler, the better.
										Simple models have the advantage of being lighter
										(ocupying less memory) and faster.
										Moreover, they are more explainable/interpretable: if they make a prediction error,
										we have a better chance
										of actually understanding what went wrong (*cough* ChatGPT *cough*)
										<a href="#footnotes" class="tooltip bottom" data-tooltip='Neural networks are sometimes called "black-box models" partly because of their lack of interpretability.'>9</a>.
									</p>
									<p>
										In terms of results, we have observed that our
										method compares favorably to the traditional ReLU networks,
										with the improvement being more pronounced for simpler/smaller networks.
									</p>

									<h3>The Second Project: Deep Spline Networks with Controlled Lipschitz Constant</h3>
									
									<p>
										In this work, we modified "slightly" the regularization term used in the previous work&mdash;the new
										version being called "second-order bounded variation"&mdash;and showed that this allowed us
										to control the Lipschitz constant of the network,
										which measures how robust the network's predictions are to small changes
										in the input. This is very important to ensure the reliability of neural nets.
										For example, we dont't want self-driving cars to not be able to detect stop signs
										if someone graffitied them or pasted small stickers on top.
										Unfortunately, this has been shown to happen with standard neural nets
										<a href="https://arxiv.org/pdf/1707.08945.pdf">in this paper</a>: a clear example
										of how a lack of robustness can lead to tragic consequences.
									</p>

									<hr class="major" />

									<h2>Publications</h2>

									<div id="deepspline-pubs" class="pubs">
									</div>
									
									<hr class="major" />

									<h2>Impact</h2>

									<p>
										The foundational work on deep spline neural networks has been cited over 34 times in the scientific literature; the
										second work on the lipschitz control of such networks has been cited over 36 times.
									</p>

									<hr class="major" />

									<h2>Code & Technologies</h2>

									<p>
										The code for this project can be found in this
										<a href="https://github.com/joaquimcampos/Deepsplines" target="_blank">Github Repository</a>. Here, I outline a few technologies that were used:
										<ul>
											<li><a href="https://www.python.org/" target="_blank">Python</a>: Programming language.</li>
											<li><a href="https://pytorch.org/" target="_blank">Pytorch</a>: Deep learning framework.</li>
											<li><a href="https://www.gnu.org/software/bash/" target="_blank">Bash</a>: Unix shell and command language.</li>
										</ul>
									</p>

									<hr class="major" />
									
									<h2>Acknowledgments</h2>
									<p>
										I would like to thank my supervisor, Shayan Aziznejad, and Professor Michael Unser for their kindness and insight.
									</p>

									<hr class="major" />
									
									<div id="footnotes">
										<h2>Footnotes</h2>
										<p>
											[1] In technical terms, we say that our goal is to find a model that "fits the data".
										</p>
										<p>
											[2] In technical terms, we say that we are looking for a model that "generalizes well to new inputs". The problem of
											having a model that fits the data but is not able to generalize is called <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.
										</p>
										<p>
											[3] I am restricting myself here to classification for didactic purposes.
										</p>
										<p>
											[4] A pointwise operation is one for which each output element depends on a
											single input element.
										</p>
										<p>
											[5] Otherwise, the problem is <a href="https://en.wikipedia.org/wiki/Well-posed_problem">ill-posed</a>.
										</p>
										<p>
											[6] There are some extra boundary functions to make the location of the
											knots bounded to a region of interest.
										</p>
										<p>
											[7] This is not the case with ReLU basis functions because they are not localized (they are non-zero at any point to the right of the knot). Therefore, we say they are poorly-conditioned.
										</p>
										<p>
											[8] In technical terms, our method is able to enforce activation functions with
											sparse second-derivatives (in the weak sense).
										</p>
										<p>
											[9] Neural networks are sometimes called
											<a href="https://stats.stackexchange.com/questions/93705/why-are-neural-networks-described-as-black-box-models">
												"black-box models"
											</a>
											partly because of their lack of interpretability.
										</p>
									</div>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
							<nav id="menu">
								<header class="major">
									<h2>Menu</h2>
								</header>
								<ul>
									<li><a href="index.html">Home</a></li>
									<li><a href="about.html">About</a></li>
									<li>
										<span class="opener active">Projects</span>
										<ul>
											<li><a href="compression.html">Neural Image & Video Compression</a></li>
											<li><a href="radiobooks.html">Text-to-Speech App</a></li>
											<li><a href="htv.html">Hessian-Schatten Total Variation</a></li>
											<li><a id="current" href="deepsplines.html">Deepsplines</a></li>
											<li><a href="mahayana.html">Ethics in Mahayana Buddhism</a></li>
											<li><a href="madhyamaka.html">Communicating Emptiness</a></li>
										</ul>
									</li>
									<li><a href="pubs.html">Publications</a></li>
								</ul>
							</nav>

							<!-- Footer: check html/footer.html -->
							<footer id="footer">
							</footer>

							<div style="margin-bottom: 10em"></div>
						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>

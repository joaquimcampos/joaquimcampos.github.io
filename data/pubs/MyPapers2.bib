
@inproceedings{campos_content_2019,
	title = {Content adaptive optimization for neural image compression},
	copyright = {All rights reserved},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}},
	author = {Campos, Joaquim and Meierhans, Simon and Djelouah, Abdelaziz and Schroers, Christopher},
	month = jun,
	year = {2019},
	file = {Campos et al. - Content Adaptive Optimization for Neural Image Com.pdf:/home/jcampos/Zotero/storage/T6E9YBGF/Campos et al. - Content Adaptive Optimization for Neural Image Com.pdf:application/pdf},
}

@article{aziznejad_measuring_2021,
	title = {Measuring {Complexity} of {Learning} {Schemes} {Using} {Hessian}-{Schatten} {Total}-{Variation}},
	copyright = {All rights reserved},
	abstract = {In this paper, we introduce the Hessian-Schatten total-variation (HTV)—a novel seminorm that quantiﬁes the total “rugosity” of multivariate functions. Our motivation for deﬁning HTV is to assess the complexity of supervised learning schemes. We start by specifying the adequate matrixvalued Banach spaces that are equipped with suitable classes of mixednorms. We then show that HTV is invariant to rotations, scalings, and translations. Additionally, its minimum value is achieved for linear mappings, supporting the common intuition that linear regression is the least complex learning model. Next, we present closed-form expressions for computing the HTV of two general classes of functions. The ﬁrst one is the class of Sobolev functions with a certain degree of regularity, for which we show that HTV coincides with the Hessian-Schatten seminorm that is sometimes used as a regularizer for image reconstruction. The second one is the class of continuous and piecewise linear (CPWL) functions. In this case, we show that the HTV reﬂects the total change in slopes between linear regions that have a common facet. Hence, it can be viewed as a convex relaxation ( 1-type) of the number of linear regions ( 0-type) of CPWL mappings. Finally, we illustrate the use of our proposed seminorm with some concrete examples.},
	language = {en},
	urldate = {2021-12-14},
	journal = {arXiv:2112.06209},
	author = {Aziznejad, Shayan and Campos, Joaquim and Unser, Michael},
	month = dec,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Aziznejad et al. - 2021 - Measuring Complexity of Learning Schemes Using Hes.pdf:/home/jcampos/Zotero/storage/IWMCS347/Aziznejad et al. - 2021 - Measuring Complexity of Learning Schemes Using Hes.pdf:application/pdf},
}

@article{bohra_learning_2020,
	title = {Learning {Activation} {Functions} in {Deep} ({Spline}) {Neural} {Networks}},
	volume = {1},
	copyright = {All rights reserved},
	issn = {2644-1322},
	doi = {10.1109/OJSP.2020.3039379},
	abstract = {We develop an efficient computational solution to train deep neural networks (DNN) with free-form activation functions. To make the problem well-posed, we augment the cost functional of the DNN by adding an appropriate shape regularization: the sum of the second-order total-variations of the trainable nonlinearities. The representer theorem for DNNs tells us that the optimal activation functions are adaptive piecewise-linear splines, which allows us to recast the problem as a parametric optimization. The challenging point is that the corresponding basis functions (ReLUs) are poorly conditioned and that the determination of their number and positioning is also part of the problem. We circumvent the difficulty by using an equivalent B-spline basis to encode the activation functions and by expressing the regularization as an ℓ1-penalty. This results in the specification of parametric activation function modules that can be implemented and optimized efficiently on standard development platforms. We present experimental results that demonstrate the benefit of our approach.},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bohra, Pakshal and Campos, Joaquim and Gupta, Harshit and Aziznejad, Shayan and Unser, Michael},
	month = nov,
	year = {2020},
	keywords = {Activation functions, B-splines, Computational efficiency, deep learning, Deep learning, Neural networks, Neurons, regularization, sparsity, Splines (mathematics), Training},
	pages = {295--309},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/BN893BG6/Bohra et al. - 2020 - Learning Activation Functions in Deep (Spline) Neu.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/X957TIU5/9264754.html:text/html},
}

@article{aziznejad_deep_2020,
	title = {Deep {Neural} {Networks} {With} {Trainable} {Activations} and {Controlled} {Lipschitz} {Constant}},
	volume = {68},
	copyright = {All rights reserved},
	issn = {1941-0476},
	doi = {10.1109/TSP.2020.3014611},
	abstract = {We introduce a variational framework to learn the activation functions of deep neural networks. Our aim is to increase the capacity of the network while controlling an upper-bound of the actual Lipschitz constant of the input-output relation. To that end, we first establish a global bound for the Lipschitz constant of neural networks. Based on the obtained bound, we then formulate a variational problem for learning activation functions. Our variational problem is infinite-dimensional and is not computationally tractable. However, we prove that there always exists a solution that has continuous and piecewise-linear (linear-spline) activations. This reduces the original problem to a finite-dimensional minimization where an ℓ1 penalty on the parameters of the activations favors the learning of sparse nonlinearities. We numerically compare our scheme with standard ReLU network and its variations, PReLU and LeakyReLU and we empirically demonstrate the practical aspects of our framework.},
	journal = {IEEE Transactions on Signal Processing},
	author = {Aziznejad, Shayan and Gupta, Harshit and Campos, Joaquim and Unser, Michael},
	month = aug,
	year = {2020},
	keywords = {Biological neural networks, Deep learning, deep splines, Kernel, learned activations, lipschitz regularity, Minimization, representer theorem, Splines (mathematics), Standards, Training},
	pages = {4688--4699},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/7K5PF3B8/Aziznejad et al. - 2020 - Deep Neural Networks With Trainable Activations an.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/UEH9VE5E/9163082.html:text/html},
}

@inproceedings{djelouah_neural_2019,
	title = {Neural {Inter}-{Frame} {Compression} for {Video} {Coding}},
	copyright = {All rights reserved},
	doi = {10.1109/ICCV.2019.00652},
	abstract = {While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Djelouah, Abdelaziz and Campos, Joaquim and Schaub-Meyer, Simone and Schroers, Christopher},
	month = oct,
	year = {2019},
	keywords = {Codecs, Distortion, Image coding, Image reconstruction, Interpolation, Optical imaging, Video compression},
	pages = {6420--6428},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/QRDF3B4G/Djelouah et al. - 2019 - Neural Inter-Frame Compression for Video Coding.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/QKTLN9M3/9009574.html:text/html},
}

@article{campos_learning_2022,
	title = {Learning of {Continuous} and {Piecewise}-{Linear} {Functions} {With} {Hessian} {Total}-{Variation} {Regularization}},
	volume = {3},
	copyright = {All rights reserved},
	issn = {2644-1322},
	doi = {10.1109/OJSP.2021.3136488},
	abstract = {We develop a novel 2D functional learning framework that employs a sparsity-promoting regularization based on second-order derivatives. Motivated by the nature of the regularizer, we restrict the search space to the span of piecewise-linear box splines shifted on a 2D lattice. Our formulation of the infinite-dimensional problem on this search space allows us to recast it exactly as a finite-dimensional one that can be solved using standard methods in convex optimization. Since our search space is composed of continuous and piecewise-linear functions, our work presents itself as an alternative to training networks that deploy rectified linear units, which also construct models in this family. The advantages of our method are fourfold: the ability to enforce sparsity, favoring models with fewer piecewise-linear regions; the use of a rotation, scale and translation-invariant regularization; a single hyperparameter that controls the complexity of the model; and a clear model interpretability that provides a straightforward relation between the parameters and the overall learned function. We validate our framework in various experimental setups and compare it with neural networks.},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Campos, Joaquim and Aziznejad, Shayan and Unser, Michael},
	month = jan,
	year = {2022},
	keywords = {barycentric coordinates, Box splines, Junctions, Neural networks, Search problems, Signal processing, sparsity, Splines (mathematics), supervised learning, Supervised learning, TV, variational methods},
	pages = {36--48},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/H6NPAL2H/Campos et al. - 2022 - Learning of Continuous and Piecewise-Linear Functi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/S9T6Y9JU/9655475.html:text/html},
}


% Patents

@patent{Mabtech2020,
        Author = {Pol {del Aguila Pla} and Joakim Jald\'{e}n and Klas Magnusson and Daniel Pelikan and Christian Smedman},
        Title = {Systems and methods for generating a latent space residual},
        Number = {16627029},
        Url = {https://patents.google.com/patent/WO2019004913A1},
        Type = {Patent Application},
        Nationality = {International},
        Year = {2020},
}
@patent{Mabtech2017,
        Author = {Pol {del Aguila Pla} and Joakim Jald\'{e}n and Klas Magnusson
        and Daniel Pelikan and Christian Smedman},
        Title = {Method and system for analysing Fluorospot assays},
        Number = {SE 543211},
        Url = {},
        Type = {Patent},
        Nationality = {Swedish},
        Year = {2017},
}



% Theses

@mastersthesis{CamposJoaquimMScThesis,
	Author = {Joaquim {Campos}},
	School = {École Polytéchnique Fédérale de Lausanne},
	Title = {Higher-Order Regularization Methods for Supervised Learning},
	Year = {2019},
	Url = {https://github.com/joaquimcampos/joaquimcampos.github.io/blob/master/data/pubs/MSc_thesis.pdf},
	Abstract = {In the past few decades, the amount of available data has been
	growing exponentially --- a fact which is symbolized by the appearance of
	the "Big Data" industry. Devices ranging from biosensors to smartphones turn
	almost every aspect of our lives into digital information, which is now
	continuosly shared over internet and easily stored. Moreover, processing
	power continues to follow the famous "Moore's law", reliably doubling every
	∼ 2 years 1 . This combination of vast amounts of data with computing power
	makes the study of algorithms which can efectively and reliably extract
	information from data as relevant as ever. To answer the question of how to
	learn from data is precisely the goal of Machine Learning and, specifically,
	a class of algorithms known as Supervised Learning. In the parallel world of
	Signal Processing, the connection between signals in continuous and discrete
	domain has been well studied. The Nyquist-Shannon sampling theorem is an
	example of this, which establishes that all the information of a limited
	bandwidth signal can be captured with enough samples. More recently, the
	field of compressed sensing has been analysing under-determined systems,
	with the goal of acquiring and reconstructing signals when very few samples
	are available (below the Nyquist-Shannon theorem requirements). These
	algorithms exploit the sparsity of the signal in some domain to provide
	higher-quality results. Because sparsity introduces simpler and more
	interpretable solutions, it is a desirable feature for models. This two-part
	thesis develops higher-order regularization methods for supervised learning,
	while exploring model sparsity and the use of splines in merging the
	continuous and discrete worlds. In the first part (deep splines), based on
	the work of Unser et al., Gupta et al. and Debarre et al., we learn the 1D
	activation functions of a neural network, together with the rest of the
	parameters. The deep spline module will be evaluated in an area classification
	problem and a model sparsification method will be introduced. In the second
	part, we develop a novel 2D learning framework, using a Hessian-Schatten
	regularization term. Unlike in Lefkimmiatis et al., the problem will be
	treated in its continuous formulation and the advantages of learning over
	function spaces will be discussed. Finally, this framework will be applied to
	the tasks of reconstruction, data fitting and "2D super-resolution".
	},
}

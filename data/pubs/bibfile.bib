
@inproceedings{campos_content_2019,
	title = {Content adaptive optimization for neural image compression},
	copyright = {All rights reserved},
	booktitle = {Proceedings of the {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}) {Workshops}},
	author = {Campos, Joaquim and Meierhans, Simon and Djelouah, Abdelaziz and Schroers, Christopher},
	month = jun,
	year = {2019},
	file = {Campos et al. - Content Adaptive Optimization for Neural Image Com.pdf:/home/jcampos/Zotero/storage/T6E9YBGF/Campos et al. - Content Adaptive Optimization for Neural Image Com.pdf:application/pdf},
}

@article{aziznejad_measuring_2021,
	title = {Measuring {Complexity} of {Learning} {Schemes} {Using} {Hessian}-{Schatten} {Total}-{Variation}},
	copyright = {All rights reserved},
	abstract = {In this paper, we introduce the Hessian-Schatten total-variation (HTV)—a novel seminorm that quantiﬁes the total “rugosity” of multivariate functions. Our motivation for deﬁning HTV is to assess the complexity of supervised learning schemes. We start by specifying the adequate matrixvalued Banach spaces that are equipped with suitable classes of mixednorms. We then show that HTV is invariant to rotations, scalings, and translations. Additionally, its minimum value is achieved for linear mappings, supporting the common intuition that linear regression is the least complex learning model. Next, we present closed-form expressions for computing the HTV of two general classes of functions. The ﬁrst one is the class of Sobolev functions with a certain degree of regularity, for which we show that HTV coincides with the Hessian-Schatten seminorm that is sometimes used as a regularizer for image reconstruction. The second one is the class of continuous and piecewise linear (CPWL) functions. In this case, we show that the HTV reﬂects the total change in slopes between linear regions that have a common facet. Hence, it can be viewed as a convex relaxation ( 1-type) of the number of linear regions ( 0-type) of CPWL mappings. Finally, we illustrate the use of our proposed seminorm with some concrete examples.},
	language = {en},
	urldate = {2021-12-14},
	journal = {arXiv:2112.06209},
	author = {Aziznejad, Shayan and Campos, Joaquim and Unser, Michael},
	month = dec,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Aziznejad et al. - 2021 - Measuring Complexity of Learning Schemes Using Hes.pdf:/home/jcampos/Zotero/storage/IWMCS347/Aziznejad et al. - 2021 - Measuring Complexity of Learning Schemes Using Hes.pdf:application/pdf},
}

@article{bohra_learning_2020,
	title = {Learning {Activation} {Functions} in {Deep} ({Spline}) {Neural} {Networks}},
	volume = {1},
	copyright = {All rights reserved},
	issn = {2644-1322},
	doi = {10.1109/OJSP.2020.3039379},
	abstract = {We develop an efficient computational solution to train deep neural networks (DNN) with free-form activation functions. To make the problem well-posed, we augment the cost functional of the DNN by adding an appropriate shape regularization: the sum of the second-order total-variations of the trainable nonlinearities. The representer theorem for DNNs tells us that the optimal activation functions are adaptive piecewise-linear splines, which allows us to recast the problem as a parametric optimization. The challenging point is that the corresponding basis functions (ReLUs) are poorly conditioned and that the determination of their number and positioning is also part of the problem. We circumvent the difficulty by using an equivalent B-spline basis to encode the activation functions and by expressing the regularization as an ℓ1-penalty. This results in the specification of parametric activation function modules that can be implemented and optimized efficiently on standard development platforms. We present experimental results that demonstrate the benefit of our approach.},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Bohra, Pakshal and Campos, Joaquim and Gupta, Harshit and Aziznejad, Shayan and Unser, Michael},
	month = nov,
	year = {2020},
	keywords = {Neural networks, B-splines, deep learning, Splines (mathematics), Training, sparsity, regularization, Computational efficiency, Activation functions, Deep learning, Neurons},
	pages = {295--309},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/BN893BG6/Bohra et al. - 2020 - Learning Activation Functions in Deep (Spline) Neu.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/X957TIU5/9264754.html:text/html},
}

@article{aziznejad_deep_2020,
	title = {Deep {Neural} {Networks} {With} {Trainable} {Activations} and {Controlled} {Lipschitz} {Constant}},
	volume = {68},
	copyright = {All rights reserved},
	issn = {1941-0476},
	doi = {10.1109/TSP.2020.3014611},
	abstract = {We introduce a variational framework to learn the activation functions of deep neural networks. Our aim is to increase the capacity of the network while controlling an upper-bound of the actual Lipschitz constant of the input-output relation. To that end, we first establish a global bound for the Lipschitz constant of neural networks. Based on the obtained bound, we then formulate a variational problem for learning activation functions. Our variational problem is infinite-dimensional and is not computationally tractable. However, we prove that there always exists a solution that has continuous and piecewise-linear (linear-spline) activations. This reduces the original problem to a finite-dimensional minimization where an ℓ1 penalty on the parameters of the activations favors the learning of sparse nonlinearities. We numerically compare our scheme with standard ReLU network and its variations, PReLU and LeakyReLU and we empirically demonstrate the practical aspects of our framework.},
	journal = {IEEE Transactions on Signal Processing},
	author = {Aziznejad, Shayan and Gupta, Harshit and Campos, Joaquim and Unser, Michael},
	month = aug,
	year = {2020},
	keywords = {Splines (mathematics), Training, Minimization, representer theorem, Kernel, Deep learning, Biological neural networks, deep splines, learned activations, lipschitz regularity, Standards},
	pages = {4688--4699},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/7K5PF3B8/Aziznejad et al. - 2020 - Deep Neural Networks With Trainable Activations an.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/UEH9VE5E/9163082.html:text/html},
}

@inproceedings{djelouah_neural_2019,
	title = {Neural {Inter}-{Frame} {Compression} for {Video} {Coding}},
	copyright = {All rights reserved},
	doi = {10.1109/ICCV.2019.00652},
	abstract = {While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.},
	booktitle = {Proceedings of the {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Djelouah, Abdelaziz and Campos, Joaquim and Schaub-Meyer, Simone and Schroers, Christopher},
	month = oct,
	year = {2019},
	keywords = {Image reconstruction, Image coding, Interpolation, Codecs, Distortion, Optical imaging, Video compression},
	pages = {6420--6428},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/QRDF3B4G/Djelouah et al. - 2019 - Neural Inter-Frame Compression for Video Coding.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/QKTLN9M3/9009574.html:text/html},
}

@article{campos_learning_2022,
	title = {Learning of {Continuous} and {Piecewise}-{Linear} {Functions} {With} {Hessian} {Total}-{Variation} {Regularization}},
	volume = {3},
	copyright = {All rights reserved},
	issn = {2644-1322},
	doi = {10.1109/OJSP.2021.3136488},
	abstract = {We develop a novel 2D functional learning framework that employs a sparsity-promoting regularization based on second-order derivatives. Motivated by the nature of the regularizer, we restrict the search space to the span of piecewise-linear box splines shifted on a 2D lattice. Our formulation of the infinite-dimensional problem on this search space allows us to recast it exactly as a finite-dimensional one that can be solved using standard methods in convex optimization. Since our search space is composed of continuous and piecewise-linear functions, our work presents itself as an alternative to training networks that deploy rectified linear units, which also construct models in this family. The advantages of our method are fourfold: the ability to enforce sparsity, favoring models with fewer piecewise-linear regions; the use of a rotation, scale and translation-invariant regularization; a single hyperparameter that controls the complexity of the model; and a clear model interpretability that provides a straightforward relation between the parameters and the overall learned function. We validate our framework in various experimental setups and compare it with neural networks.},
	journal = {IEEE Open Journal of Signal Processing},
	author = {Campos, Joaquim and Aziznejad, Shayan and Unser, Michael},
	month = jan,
	year = {2022},
	keywords = {Neural networks, Signal processing, Splines (mathematics), TV, sparsity, Box splines, barycentric coordinates, Junctions, Search problems, supervised learning, Supervised learning, variational methods},
	pages = {36--48},
	file = {IEEE Xplore Full Text PDF:/home/jcampos/Zotero/storage/H6NPAL2H/Campos et al. - 2022 - Learning of Continuous and Piecewise-Linear Functi.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jcampos/Zotero/storage/S9T6Y9JU/9655475.html:text/html},
}

@article{goujon_stable_2022,
	title = {Stable {Parametrization} of {Continuous} and {Piecewise}-{Linear} {Functions}},
	copyright = {All rights reserved},
	abstract = {Rectiﬁed-linear-unit (ReLU) neural networks, which play a prominent role in deep learning, generate continuous and piecewise-linear (CPWL) functions. While they provide a powerful parametric representation, the mapping between the parameter and function spaces lacks stability. In this paper, we investigate an alternative representation of CPWL functions that relies on local hat basis functions. It is predicated on the fact that any CPWL function can be speciﬁed by a triangulation and its values at the grid points. We give the necessary and suﬃcient condition on the triangulation (in any number of dimensions) for the hat functions to form a Riesz basis, which ensures that the link between the parameters and the corresponding CPWL function is stable and unique. In addition, we provide an estimate of the 2 → L2 condition number of this local representation. Finally, as a special case of our framework, we focus on a systematic parametrization of Rd with control points placed on a uniform grid. In particular, we choose hat basis functions that are shifted replicas of a single linear box spline. In this setting, we prove that our general estimate of the condition number is optimal. We also relate our local representation to a nonlocal one based on shifts of a causal ReLU-like function.},
	language = {en},
	urldate = {2022-03-16},
	journal = {arXiv:2203.05261},
	author = {Goujon, Alexis and Campos, Joaquim and Unser, Michael},
	month = mar,
	year = {2022},
	keywords = {41A99, G.1.1, G.1.2, Mathematics - Numerical Analysis},
	file = {Goujon et al. - 2022 - Stable Parametrization of Continuous and Piecewise.pdf:/home/jcampos/Zotero/storage/RQXWYU7M/Goujon et al. - 2022 - Stable Parametrization of Continuous and Piecewise.pdf:application/pdf},
}

% Theses

@mastersthesis{campos_higher-order_2019,
	title = {Higher-{Order} {Regularization} {Methods} for {Supervised} {Learning}},
	copyright = {All rights reserved},
	url = {https://github.com/joaquimcampos/joaquimcampos.github.io/blob/master/data/pubs/MSc_thesis.pdf},
	abstract = {In the past few decades, the amount of available data has been growing exponentially — a fact which is symbolized by the appearance of the "Big Data" industry. Devices ranging from biosensors to smartphones turn almost every aspect of our lives into digital information, which is now continuosly shared over internet and easily stored. Moreover, processing power continues to follow the famous "Moore's law", reliably doubling every ∼ 2 years 1 . This combination of vast amounts of data with computing power makes the study of algorithms which can efectively and reliably extract information from data as relevant as ever. To answer the question of how to learn from data is precisely the goal of Machine Learning and, specifically, a class of algorithms known as Supervised Learning. In the parallel world of Signal Processing, the connection between signals in continuous and discrete domain has been well studied. The Nyquist-Shannon sampling theorem is an example of this, which establishes that all the information of a limited bandwidth signal can be captured with enough samples. More recently, the field of compressed sensing has been analysing under-determined systems, with the goal of acquiring and reconstructing signals when very few samples are available (below the Nyquist-Shannon theorem requirements). These algorithms exploit the sparsity of the signal in some domain to provide higher-quality results. Because sparsity introduces simpler and more interpretable solutions, it is a desirable feature for models. This two-part thesis develops higher-order regularization methods for supervised learning, while exploring model sparsity and the use of splines in merging the continuous and discrete worlds. In the first part (deep splines), based on the work of Unser et al., Gupta et al. and Debarre et al., we learn the 1D activation functions of a neural network, together with the rest of the parameters. The deep spline module will be evaluated in an area classification problem and a model sparsification method will be introduced. In the second part, we develop a novel 2D learning framework, using a Hessian-Schatten regularization term. Unlike in Lefkimmiatis et al., the problem will be treated in its continuous formulation and the advantages of learning over function spaces will be discussed. Finally, this framework will be applied to the tasks of reconstruction, data fitting and "2D super-resolution".},
	school = {École Polytéchnique Fédérale de Lausanne},
	author = {Campos, Joaquim},
	year = {2019},
}

% Patents

@patent{schroers_systems_2021,
	title = {Systems and methods for generating a latent space residual},
	copyright = {All rights reserved},
	url = {https://patents.google.com/patent/US11012718B2/en},
	abstract = {Systems and methods are disclosed for generating a latent space residual. A computer-implemented method may use a computer system that includes non-transient electronic storage, a graphical user interface, and one or more physical computer processors. The computer-implemented method may include: obtaining a target frame, obtaining a reconstructed frame, encoding the target frame into a latent space to generate a latent space target frame, encoding the reconstructed frame into the latent space to generate a latent space reconstructed frame, and generating a latent space residual based on the latent space target frame and the latent space reconstructed frame.},
	nationality = {US},
	number = {11012718},
	author = {Schroers, Christopher and Campos, Joaquim and Djelouah, Abdelaziz and Yuanyi, XUE and Varis Doggett, Erika and Mcphillen, Jared and Labrozzi, Scott},
	month = may,
	year = {2021},
	note = {Type: Patent Application},
}

@patent{schroers_systems_2021-1,
	title = {Systems and methods for reconstructing frames},
	copyright = {All rights reserved},
	url = {https://patents.google.com/patent/US10972749B2/en},
	abstract = {Systems and methods are disclosed for reconstructing a frame. A computer-implemented method may use a computer system that includes non-transient electronic storage, a graphical user interface, and one or more physical computer processors. The computer-implemented method may include: obtaining one or more reference frames from non-transient electronic storage, generating one or more displacement maps based on the one or more reference frames and a target frame with the physical computer processor, generating one or more warped frames based on the one or more reference frames and the one or more displacement maps with the physical computer processor, obtaining a conditioned reconstruction model from the non-transient electronic storage, and generating one or more blending coefficients and one or more reconstructed displacement maps by applying the one or more displacement maps, the one or more warped frames, and a target frame to the conditioned reconstruction model with the physical computer processor.},
	nationality = {US},
	number = {10972749},
	author = {Schroers, Christopher and Campos, Joaquim and Djelouah, Abdelaziz and Yuanyi, XUE and Varis Doggett, Erika and Mcphillen, Jared and Labrozzi, Scott},
	month = apr,
	year = {2021},
	note = {Type: Patent Application},
}

@patent{schroers_content_2021,
	title = {Content adaptive optimization for neural data compression},
	copyright = {All rights reserved},
	url = {https://patents.google.com/patent/US11057634B2/en},
	abstract = {A data processing system includes a computing platform having a hardware processor and a memory storing a data compression software code. The hardware processor executes the data compression software code to receive a series of compression input data and encode a first compression input data of the series to a latent space representation of the first compression input data. The data compression software code further decodes the latent space representation to produce an input space representation of the first compression input data corresponding to the latent space representation, and generates f refined latent values for re-encoding the first compression input data based on a comparison of the first compression input data with its input space representation. The data compression software code then re-encodes the first compression input data using the refined latent values to produce a first compressed data corresponding to the first compression input data.},
	nationality = {US},
	number = {11057634},
	author = {Schroers, Christopher and Meierhans, Simon and Campos, Joaquim and Mcphillen, Jared and Djelouah, Abdelaziz and Varis Doggett, Erika and Labrozzi, Scott and Yuanyi, XUE},
	month = jul,
	year = {2021},
	note = {Type: Patent Application},
}

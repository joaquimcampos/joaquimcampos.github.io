<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Hessian-Schatten Total Variation</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<div id="topbar"></div>

						<div class="inner">

							<!-- Content -->
								<section id="project">
									<header class="main">
										<h1>Hessian-Schatten Total Variation <br>
										<span>@BIG.EPFL</span>
										</h1>
									</header>
									
									<div class="flex-box project-banner">
										<a id="htv-img" class="flex-image" href="https://ieeexplore.ieee.org/document/9655475" target="_blank">
											<img src="images/htv1.png" alt="Face GT">
											<img src="images/htv2.png" alt="Face HTV">
											<img src="images/htv3.png" alt="Face HTV Sparser">
										</a>
									</div>

									<header class="major">
										<h2>Introduction</h2>
									</header>
									<!-- Introduction: Supervised Learning and Regularization -->
									<p>
										Suppose you have a dataset \({\mathrm D}\) that consists of sequence of \(M\) images
    									of mountains together with their
    									<a href="https://en.wikipedia.org/wiki/Topographic_prominence" target="_blank">prominence</a> in meters (I'll just call it height, because being precise about mountain geography is not the point here). Mathematically, we write this statement as \({\mathrm D} = \lbrace{\mathbf x}_m, y_m\rbrace _{m=1}^M\), where each datapoint \(({\mathbf x}_m, y_m)\) is an image-height pair, as depicted below.
									</p>
									<div class="flex-box body-box">
										<div id="matterhorn-img">
											<img src="images/matterhorn.png" alt="Matterhorn">
										</div>
									</div>
									<p>
										In supervised learning, our goal is to find a model \(f\) that predicts fairly well the height \(y_m\) of each mountain in our dataset from its depiction \({\mathbf x}_m\)
										(mathematically, \(f({\bf x}_m) = \hat{y}_m \approx y_m\))
										<a href="#footnotes" class="tooltip tooltip-narrow bottom" data-tooltip='We call this "fitting the data".'>1</a>, and that is able to generalize to new inputs: that is, that can accurately predict
										the heights of mountains from their respective images even when it has notfitting
										seen those images before in the training data.
									</p>
									<p>
										In general, this is a very difficult task to solve because even though there are infinite models that fit the data well enough, very few of them actually make sense and are able
										to generalize. To tackle this issue, we need to introduce some constraints.
									</p>
									<p>
										First, if we know something about the data distribution, then we can use
										that information. For example, if the data is the result of a linear mapping plus
										some measurement noise (mathematically, \(y = {\mathbf w}^T{\mathbf x} + b + \epsilon\)), then we can just search within this family of linear models; in this way,
										an a priori complicated problem is reduced to a simple one of finding just a few parameters (\({\mathbf w}\) and \(b\)).
										Alternatively, we might restrict ourselves to a family
										of models that we know to contain elements that perform well for similar tasks
										and introduce some information about what kind of models we're looking for in the form of
										<a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" target="_blank">regularization</a>.
										This serves to make the problem solvable or to favor models that can generalize well
										<a href="#footnotes" class="tooltip bottom" data-tooltip='In technical terms, regularization is used to make the problem well-posed or to prevent overfitting (cf. footnote).'>2</a>.
										A very common practice is to use regularization terms favor simple models
										from the search space. This follows from the Occam's razor principle:
										as long as it can do the job, the simpler, the better.
										Simple models have the advantage of being lighter (ocupying less memory),
										faster, and being more robust—in
										that small or irrelevant changes in the image are less likely to screw up the predictions
										<a href="#footnotes" class="tooltip tooltip-narrowest bottom" data-tooltip='Cf. footnote for an example where lack of robustness can lead to tragic consequences.'>3</a>. Moreover, they are more are more explainable/interpretable: if they make a prediction error, we have a better chance of actually understanding what went wrong (*cough* ChatGPT *cough*)
										<a href="#footnotes" class="tooltip bottom" data-tooltip='Neural networks are sometimes called "black-box models" (cf. footnote) partly because of their lack of interpretability.'>4</a>.
									</p>
									<p>
										A family of models that has had a lot of success in the past few years are neural networks, which are part of the larger family of continuous and piecewise-linear functions (functions whose graph is composed of straight-line segments like this &#x1F4C9;)
										<a href="#footnotes" class="tooltip tooltip-narrow top" data-tooltip='Those with ReLU-like activations, which are the most used ones.'>5</a>. But left to their own devices, neural nets lead to a <a href="https://arxiv.org/pdf/1402.1869.pdf" target="_blank">very large number of linear regions</a>, which makes them less robust and interpretable.    
									</p>

									<hr class="major" />

									<header class="major">
										<h2>This Project, Explained</h2>
									</header>
									<p>
										In this project, my colleagues at the Biomedical Imaging Group and I developed a method for learning continuous and piecewise-linear models characterized by few regions and a clear relationship with their parameters, rendering them interpretable. To achieve this, we employed a novel regularizer known as Hessian-Schatten Total Variation, designed to enforce sparse second derivatives, and restricted our search to a family of models composed of 'lego pieces' (basis functions) called <em>box splines</em>. In the following image you can see (on the left) one of these lego pieces, and (on the right)
										an example of a learned model that (in simple terms) results from placing different lego
										pieces with different heights at different locations.
									</p>
									<div class="flex-box body-box">
										<div id="boxspline-row">
											<img src="images/boxspline.png" alt="Box Spline">
											<img src="images/boxspline_model.png" alt="Piecewise-Linear Model">
										</div>
									</div>
									<p>
										With this method we are able to regulate the level of simplicity (sparsity) in our models by adjusting a single parameter—the regularization weight. This is shown in <a href="#htv-img">top image in this page</a>,
										where we trained our models on a face dataset with \(M = 5000\) samples. On the left, you can see the ground-truth; in the middle and right, you can see our learned models with different regularization weights, respectively. The model in the middle, which has a lower regularization weight, can fit the data better but has more linear regions than the model on the right.
									</p>

									<hr class="major" />

									<h2>Publications</h2>

									<div id="htv-pubs" class="pubs">
									</div>
									
									<hr class="major" />

									<h2>Impact</h2>

									<p>
										This work relates to three papers that, in total, have been cited over 19 times in the scientific literature. In particular, these results were used in two papers [<a href="https://arxiv.org/abs/2210.04077">1</a>,
										<a href="https://arxiv.org/abs/2302.12554">2</a>] of mathematician
										<a href="https://en.wikipedia.org/wiki/Luigi_Ambrosio">Luigi Ambrosio</a>, a
										leading expert in the calculus of variations and geometric measure theory, and
										doctoral advisor of
										<a href="https://en.wikipedia.org/wiki/Fields_Medal">Fields Medal</a>
										winner
										<a href="https://en.wikipedia.org/wiki/Alessio_Figalli">Alessio Figalli</a>.
									</p>

									<hr class="major" />

									<h2>Code & Technologies</h2>

									<p>
										The code for this project can be found in this
										<a href="https://github.com/joaquimcampos/HTV-Learn" target="_blank">Github Repository</a>. Here, I outline a few technologies that were used:
										<ul>
											<li><a href="https://www.python.org/" target="_blank">Python</a>: Programming language.</li>
											<li><a href="https://pytorch.org/" target="_blank">Pytorch</a>: Deep learning framework.</li>
											<li><a href="https://www.gnu.org/software/bash/" target="_blank">Bash</a>: Unix shell and command language.</li>
										</ul>
									</p>

									<hr class="major" />
									
									<h2>Acknowledgments</h2>
									<p>
										I would like to thank my supervisor, Shayan Aziznejad, and Professor Michael Unser for their kindness and insight.
									</p>

									<hr class="major" />
									
									<div id="footnotes">
										<h2>Footnotes</h2>
										<p>
											[1] We call this "fitting the data".
										</p>
										<p>
											[2] In technical terms, regularization is used to make the problem <a href="https://en.wikipedia.org/wiki/Well-posed_problem" target="_blank">well-posed</a> or to prevent <a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank">overfitting</a>.
										</p>
										<p>
											[3] <a href="https://arxiv.org/pdf/1707.08945.pdf" target="_blank">
											Here </a> you can see an example where lack of robustness can lead to tragic consequences.
										</p>
										<p>
											[4] Neural networks are sometimes called
											<a href="https://stats.stackexchange.com/questions/93705/why-are-neural-networks-described-as-black-box-models" target="_blank">
												"black-box models"
											</a> partly because of their lack of interpretability.
										</p>
										<p>
											[5] Those with ReLU-like activations, which are the most used ones.
										</p>
									</div>

								</section>
						</div>

						<div id="footer"></div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">
							<!-- Menu -->
							<nav id="menu">
								<ul>
									<li><a href="index.html">Home</a></li>
									<li><a href="about.html">About</a></li>
									<li>
										<a class="opened active" href="projects.html">Projects</a>
										<ul>
											<li><a href="compression.html">Neural Image & Video Compression</a></li>
											<li><a href="radiobooks.html">Text-to-Speech App</a></li>
											<li><a id="current" href="htv.html">Hessian-Schatten Total Variation</a></li>
											<li><a href="deepsplines.html">Deepsplines</a></li>
											<li><a href="mahayana.html">Ethics in Mahayana Buddhism</a></li>
											<li><a href="madhyamaka.html">Communicating Emptiness</a></li>
										</ul>
									</li>
									<li><a href="pubs.html">Publications</a></li>
								</ul>
							</nav>
						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>

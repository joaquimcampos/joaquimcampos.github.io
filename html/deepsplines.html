<h2 class="new-project">Deep Spline Neural Networks</h2>

<h3 style="margin: 0;">
    <a href="https://bigwww.epfl.ch/">@EPFL.BIG</a> |
    <a href="https://github.com/joaquimcampos/Deepsplines">
        Github
        <github-logo></github-logo>
    </a>
</h3>

<a class="row-deepsplines" href="https://ieeexplore.ieee.org/document/9264754">
    <img id="deepsplines" src="assets/images/deepsplines.png" alt="Deepsplines">
</a>

<div class="last-element">

<h4 class="subsection">Introduction: Supervised Learning</h4>

<p>
    <i>Note: It might be useful to read <a href="#/htv">this project</a>'s introduction too</i>.
</p>
<p style="margin-bottom: 0">
    Suppose we have a dataset \({\mathrm D}\) that consists of sequence of \(M\) images
    of cats and dogs together with their respective labels.
    Mathematically, we write this statement as \({\mathrm D} = \lbrace{\mathbf x}_m, y_m\rbrace _{m=1}^M\),
    where \(y_m = 0\) if \({\mathbf x}_m\) represents a cat, and \(y_m = 1\) otherwise,
    as depicted below.
</p>
<div class="row-cat-dog">
    <img src="assets/images/cat.png" alt="Cat">
    <img src="assets/images/dog.png" alt="Dog">
</div>
<p>
    In supervised learning, our goal is to find a model \(f\) that makes few classification mistakes both for the images in our dataset <a href="" ng-click="scrollToElement('footnote-fitting')">[1]</a>
    and also new images that it has not seen before in the training data
    [<a href="" ng-click="scrollToElement('footnote-overfitting')">2</a>,
    <a href="" ng-click="scrollToElement('footnote-classification')">3</a>].
    (In the cat-dog example, the output of our model can be a vector of two values, <i>i.e.</i> \(f({\mathbf x}_m) = (y_0, y_1)\), and the predictions \({\hat y}_m\) be assigned to \({\hat y}_m = 0\) if \(y_0 >= y_1\), and \({\hat y}_m = 1\) otherwise.)
</p>
<h5 class="subsubsection">Neural Networks</h5>
<p>
    A very standard way of solving supervised learning problems is to use neural networks.
    Neural nets are sequences of simple linear transformations and pointwise
    nonlinearities called activation functions <a href="" ng-click="scrollToElement('footnote-pointwise')">[4]</a>.
    In mathematical terms, this is written as
    \[
        {\mathbf f}_{\boldsymbol{\Theta }}({\mathbf x}):
        {\mathbf W}_L \circ \cdots \circ {\boldsymbol{\sigma }}_\ell \circ {\mathbf W}_\ell \circ \dots \circ {\boldsymbol{\sigma }}_1 \circ {\mathbf W}_1 ({\mathbf x}),
    \]
    where \(L\) is the depth of the neural net and \({\boldsymbol \Theta}\) is a list of all the parameters that are adjusted during the learning process.
    The most popular neural networks use piecewise-linear activation functions called
    <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLUs</a>. With such kind of activations,
    the whole network becomes a multidimensional piecewise-linear function &#x1F4C9;.
</p>

<h4 class="subsection">This Project, Simply Explained</h4>
<p>
    Usually, only the linear transformations are adjusted learned the training, and
    the nonlinearities are fixed.
    So, piecewise linear networks work very well, but we would also like that
    the activation functions themselves would be learnable modules.
    But this would make the problem not solvable.
    What if we could add a regularization term for which there are guarantees
    that there are optimal solutions that are piecewise-linear. Then, we could
    just focus on piecewise-linear functions.
    This is called second-order total variation \({\rm TV}^{(2)}\).
    But we need a way to parametrize the functions.
    We actually control the simplicity (sparsity) of the activation functions,
    by having a regularization weight that can control the number of knots.
</p>
<p>
    Our lego pieces now are piece-wise linear functions called B(1)-splines
    $$
    \begin{align*} \varphi _k(x)=\beta ^1(x-k), \text{ for } k_{\min } < k < k_{\max}, \tag{10} \end{align*} 
    $$
    You can see how several lego pieces work together to form a parametrized activation function
    <a href="" ng-click="scrollToElement('deepsplines')">above</a> (there are just some extra boundary
    functions to make the location of the knots bounded to a region of interest).
</p>
<p>
    Abstract: <br>
    We develop an efficient computational solution to train deep neural networks (DNN) with
    free-form activation functions. To make the problem well-posed, we augment the cost functional of the DNN
    by adding an appropriate shape regularization: the sum of the second-order total-variations of the trainable
    nonlinearities. The representer theorem for DNNs tells us that the optimal activation functions are adaptive
    piecewise-linear splines, which allows us to recast the problem as a parametric optimization. The challenging
    point is that the corresponding basis functions (ReLUs) are poorly conditioned and that the determination of
    their number and positioning is also part of the problem. We circumvent the difficulty by using an equivalent
    B-spline basis to encode the activation functions and by expressing the regularization as an 1 -penalty. This
    results in the specification of parametric activation function modules that can be implemented and optimized
    efficiently on standard development platforms. We present experimental results that demonstrate the benefit
    of our approach.
    <br> <br>
    Conclusion: <br>
    We have presented an efficient computational solution to
    train deep neural networks with learnable activation functions.
    Specifically, we have focused on deep spline networks. They
    form a superset of the traditional ReLU networks and are
    known to be optimal with respect to the second-order total
    variation of the adjustable nonlinearities. We have tackled
    the resulting difficult joint-optimization problem by represent-
    ing the linear-spline nonlinearities in terms of B-spline basis.
    functions and by expressing the second-order total-variation
    regularization as an l1 -penalty, thus unifying the paramet-
    ric and functional approaches for the learning of activation
    functions. The proposed B-spline representation was instru-
    mental in making the training of the DNN computationally
    feasible. Indeed, any computation concerning the activation
    functions involves only two basis elements per data point.
    Finally, we have demonstrated the benefits of our framework
    through experiments in the context of classification and de-
    convolution problems. In particular, we have observed that our
    method compares favorably to the traditional ReLU networks,
    the improvement being more pronounced for simpler/smaller
    networks.
    <br> <br> <br>
    Lipschitz Abstract: <br>
    We introduce a variational framework to learn the
    activation functions of deep neural networks. Our aim is to increase
    the capacity of the network while controlling an upper-bound of
    the actual Lipschitz constant of the input-output relation. To that
    end, we first establish a global bound for the Lipschitz constant of
    neural networks. Based on the obtained bound, we then formulate
    a variational problem for learning activation functions. Our vari-
    ational problem is infinite-dimensional and is not computationally
    tractable. However, we prove that there always exists a solution
    that has continuous and piecewise-linear (linear-spline) activations.
    This reduces the original problem to a finite-dimensional minimiza-
    tion where an l1 penalty on the parameters of the activations favors
    the learning of sparse nonlinearities. We numerically compare our
    scheme with standard ReLU network and its variations, PReLU
    and LeakyReLU and we empirically demonstrate the practical
    aspects of our framework.
    <br> <br>
    Lipschitz Conclusion: <br>
    In this paper, we have introduced a variational framework to
    learn the activations of a deep neural network while controlling
    its global Lipschitz regularity. We have considered neural net-
    works with second-order bounded-variation activations and we
    provided a global bound for their Lipschitz constants. We have
    showed that the solution of our proposed variational problem
    exists and is in the form of a deep-spline network with continuous
    piecewise linear activation functions. Our future work in this
    direction is to explore how the simplification of architectures can
    be compensated by the deployment of more complex activations.
</p>
<h4><b>Impact</b></h4>
<p>
This work has been cited over 65 times in scientific publications
</p>

<p style="margin-top: 20px; margin-bottom: 10px">
    <b>Publications</b>
</p>
<div ng-controller="DeepSplinesPubsCtrl">
    <pubs-no-headers></pubs-no-headers>
</div>



<hr style="margin-top: 40px;">
<h5>Footnotes</h5>
<p style="margin-bottom: 0px;" id="footnote-fitting">
    [1] We say the model "fits the data".
</p>
<p style="margin-bottom: 0px;" id="footnote-overfitting">
    [2] We say the model "generalizes well to new inputs". The problem of
    having a model that fits the data but is not able to generalize is called
    <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.
</p>
<p style="margin-bottom: 0px;" id="footnote-classification">
    [3] I am restricting myself here to classification for didactic purposes.
</p>
<p style="margin-bottom: 0px;" id="footnote-pointwise">
    [4] This means that each output element depends on a single input one.
</p>
</div>

<div class="top-element full-width-center">
    <div id="deepsplines-div" class="project-icon">
        <div class="project-title">
            <h2>Deep Spline Neural Networks
                <a href="https://bigwww.epfl.ch/">@EPFL.BIG</a>
            </h2>
        </div>
        <div class="full-width-center">
            <div class="github-button">
                <a href="https://github.com/joaquimcampos/Deepsplines">
                    Github
                    <github-logo></github-logo>
                </a>
            </div>
        </div>
        <div class="full-width-center">
            <a id="deepsplines-row" class="project-img-link brown-bg" href="https://ieeexplore.ieee.org/document/9264754">
                <img id="deepsplines" src="assets/images/deepsplines.png" alt="Deepsplines">
            </a>
        </div>
    </div>
</div>

<h4 class="subsection">Introduction: Supervised Learning</h4>
<p>
    <i>Note: It might be useful to read <a href="#/htv">this project</a>'s introduction too</i>.
</p>
<p style="margin-bottom: 0">
    Suppose we have a dataset \({\mathrm D}\) that consists of sequence of \(M\) images
    of cats and dogs, together with their respective labels.
    Mathematically, we write this statement as \({\mathrm D} = \lbrace{\mathbf x}_m, y_m\rbrace _{m=1}^M\),
    where \(y_m = 0\) if \({\mathbf x}_m\) represents a cat, and \(y_m = 1\) otherwise,
    as depicted below.
</p>
<div class="full-width-center">
    <div id="cat-dog-row" class="center-img-div">
        <img src="assets/images/cat.png" alt="Cat">
        <img src="assets/images/dog.png" alt="Dog">
    </div>
</div>
<p>
    In supervised learning, our goal is to find a model \(f\) that makes few classification mistakes for the images in our dataset <a href="" ng-click="scrollToElement('footnote-fitting')">[1]</a>
    as well as new images that are not found in the training data
    [<a href="" ng-click="scrollToElement('footnote-overfitting')">2</a>,
    <a href="" ng-click="scrollToElement('footnote-classification')">3</a>].
</p>
<p>
    <i>(Mathematical details)</i> <br>
    In the cat-or-dog example, the output of our model is a two-element vector where each value represents the "confidence" in each label, and the predictions \({\hat y}_m\) are assigned according to the label with the highest confidence; that is,
    $$
    \begin{align*}
    f({\mathbf x}_m) & = (a_{m,0}, a_{m,1}) \\[5pt]
    {\hat y}_m & = 
    \begin{cases}
    0 & \text{if} \ \ a_{m,0} \geq a_{m,1} \\
    1 & \text{if} \ \ a_{m,0} < a_{m,1}
    \end{cases}
    \end{align*} 
    $$
    <i>(End of mathematics)</i>
</p>
<h5 class="subsubsection">Neural Networks</h5>
<p>
    Nowadays, neural networks are the most standard way of solving supervised learning
    problems. Neural nets are sequences of simple linear transformations and pointwise
    nonlinearities called activation functions <a href="" ng-click="scrollToElement('footnote-pointwise')">[4]</a>.
    <!-- <i>(Skippable mathematical details)</i> <br>
    In mathematical terms, this is written as
    \[
        {\mathbf f}_{\boldsymbol{\Theta }}({\mathbf x}):
        {\mathbf W}_L \circ \cdots \circ {\boldsymbol{\sigma }}_\ell \circ {\mathbf W}_\ell \circ \dots \circ {\boldsymbol{\sigma }}_1 \circ {\mathbf W}_1 ({\mathbf x}),
    \]
    where \(L\) is the depth of the neural net and \({\boldsymbol \Theta}\) is a list of all the parameters that are adjusted during the learning process. <br>
    <i>(End of mathematics)</i> -->
    The most popular activation function is the
    <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a>,
    which is a continuous and piecewise-linear (CPWL) function,
    meaning that its graph is composed of
    straight-line segments (like this &#x1F4C9;).
    Interestingly, networks with CPWL activations are CPWL functions themselves.
</p>

<h4 class="subsection">The Projects, Simply Explained</h4>
<p>
    In the standard deep learning paradigm, only the linear transformations are
    learned during the training, and the activation functions are kept fixed.
    In this project, we found a computationally cheap and fast way to also learn the
    latter. Our method showed better results compared to the baseline.
    Here, we'll see how it works.
</p>
<h5 class="subsubsection">A Bit of Theory</h5>
<p>
    In order to learn the activation functions of a neural network, we need a way
    to turn them into learnable modules that depend on discrete
    parameters, as is the case with the linear transformation
    <a href="" ng-click="scrollToElement('footnote-ill-posed')">[5]</a>.
    To do this, we have to introduce some constraints.
</p>
<p>
    Since networks with CPWL activations seem to work very well, it would be
    great if we could mantain this behaviour.
    Well, it turns out that we can modify our problem statement slightly
    by adding a <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">
    regularization</a> term called "second-order total variation",
    and have guarantees that there are optimal solutions to the problem (<i>i.e.</i>, optimal models) that actually have
    CPWL activation functions. In that case, we have a theoretical justification to restrict our search
    to such kinds of models.
    (For example, if you are looking for a very fast bike in a big bike shop, and someone tells you
    that the last Tour de France was won with a bike of this particular brand,
    then it's smarter to just try out bikes from that specific brand because
    you know that at least one of them will do the job well.)
</p>

<h5 class="subsubsection">The First Project: Deep Spline Networks</h5>
<p>
    <i>
    (There are a few important steps in getting from the theory to the actual
    algorithm that we have ommited here for the sake of simplicity.
    If you'd like to know more details, you can read
    <a href="https://ieeexplore.ieee.org/document/9264754">the paper</a>.)
    </i>
</p>
<p>
    In order to have learnable activation functions, we construct them using "lego pieces" (basis functions)
    called \(\beta ^1\)-splines, which are placed at equally-spaced locations.
    You can see <a href="" ng-click="scrollToElement('deepsplines')">above</a>
    how several lego pieces work together to form a parametrized activation function
    <a href="" ng-click="scrollToElement('footnote-boundary')">[6]</a>.
    During training, we learn the heights (coefficients) of the lego pieces together
    with the other parameters in the network.
    The models that can be found using our method are called "deep spline networks";
    they include all models with ReLU-like activations (ReLU, LeakyReLU, PReLU, etc.).
</p>
<p>
    Apart from being theoretically supported, our method has two practical key strengths:
    First, the output of the activatons at a specific location depends on (at most)
    two lego pieces because the lego pieces are localized, meaning
    that they are zero outside a small range.
    <a href="" ng-click="scrollToElement('footnote-relu-basis')">[7]</a>.
    This makes <b>our method is stable (well-conditioned), and fast to run.</b>
    Second, <b>we can control the degree of simplicity of
    our activations&mdash;as measured by the number of knots/linear regions&mdash;by
    changing the strength of the regularization applied </b>
    <a href="" ng-click="scrollToElement('footnote-sparse')">[8]</a>.
    Therefore, our approach compatible with the Occam's razor principle:
    as long as it can do the job, the simpler, the better.
    Simple models have the advantage of being lighter
    (ocupying less memory) and faster.
    Moreover, they are more explainable/interpretable: if they make a prediction error,
    we have a better chance
    of actually understanding what went wrong (*cough* ChatGPT *cough*)
    <a href="" ng-click="scrollToElement('footnote-black-box')">[9]</a>.
</p>
<p>
    In terms of results, we have observed that our
    method compares favorably to the traditional ReLU networks,
    with the improvement being more pronounced for simpler/smaller networks.
</p>

<h5 class="subsubsection">The Second Project: Deep Spline Networks with Controlled Lipschitz Constant</h5>
<p>
    In this work, we modified "slightly" the regularization term used in the previous work&mdash;the new
    version being called "second-order bounded variation"&mdash;and showed that this allowed us
    to control the Lipschitz constant of the network,
    which measures how robust the network's predictions are to small changes
    in the input. This is very important to ensure the reliability of neural nets.
    For example, we dont't want self-driving cars to not be able to detect stop signs
    if someone graffitied them or pasted small stickers on top.
    Unfortunately, this has been shown to happen with standard neural nets
    <a href="https://arxiv.org/pdf/1707.08945.pdf">here</a>: a clear example
    of how a lack of robustness can lead to tragic consequences.
</p>

<h4 style="margin-top: 40px">Impact</h4>
<p>
The first work has been cited over 28 in the scientific literature. The
second work has been cited over 34 times.
</p>

<p style="margin-top: 20px; margin-bottom: 10px">
    <b>Publications</b>
</p>
<div ng-controller="DeepSplinesPubsCtrl">
    <pubs-no-headers></pubs-no-headers>
</div>

<hr style="margin-top: 40px;">
<div class="last-element">
    <h5>Footnotes</h5>
    <p style="margin-bottom: 0px;" id="footnote-fitting">
        [1] In technical jargon, we say the model "fits the data".
    </p>
    <p style="margin-bottom: 0px;" id="footnote-overfitting">
        [2] In technical jargon, we say the model "generalizes well to new inputs". The problem of
        having a model that fits the data but is not able to generalize is called
        <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-classification">
        [3] I am restricting myself here to classification for didactic purposes.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-pointwise">
        [4] A pointwise operation is one for which each output element depends on a
        single input element.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-ill-posed">
        [5] Otherwise, the problem is 
        <a href="https://en.wikipedia.org/wiki/Well-posed_problem">ill-posed</a>.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-boundary">
        [6] There are some extra boundary functions to make the location of the
        knots bounded to a region of interest.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-relu-basis">
        [7] This is not the case with ReLU basis functions because they are not localized
        (they are non-zero at any point to the right of the knot).
        Therefore, we say they are poorly-conditioned.
    </p>
    <p style="margin-bottom: 0px;" id="footnote-sparse">
        [8] In technical terms, our method is able to enforce activation functions with
        sparse second-derivatives (in the weak sense).
    </p>
    <p style="margin-bottom: 0px;" id="footnote-black-box">
        [9] Neural networks are sometimes called
        <a href="https://stats.stackexchange.com/questions/93705/why-are-neural-networks-described-as-black-box-models">
            "black-box models"
        </a>
        partly because of their lack of interpretability.
    </p>
</div>

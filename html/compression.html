<h2 class="new-project">Neural Image and Video Compression</h2>

<h3 style="margin: 0;">
    <a href="https://studios.disneyresearch.com/researchlab/disney-research-zurich/">
        @DisneyResearch
    </a>
</h3>

<div class="row-compression">
    <a href="https://studios.disneyresearch.com/2019/10/27/neural-inter-frame-compression-for-video-coding/">
        <img src="assets/images/video_compression.png" alt="Video Compression">
    </a>
    <div></div>
    <a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/CLIC_2019/Campos_Content_Adaptive_Optimization_for_Neural_Image_Compression_CVPRW_2019_paper.html">
        <img src="assets/images/image_compression.png" alt="Image Compression">
    </a>
</div>

<div class="last-element">
<h4>This Project, Simply Explained</h4>
<p>
    What is compression?
</p>
<p>
    Let's say you would like a friend to try your home-made vegan chocolate cake.
    What is the most reliable way of doing it? Well, you bake the cake yourself,
    you drive up to your friend's house, and you give it to them.
    However, this is also the most inefficient way: A cake is heavy, and it takes time
    to bring it to your friend's house, especially if some road is cut on your way there.
</p>
<p>
    Let's look for alternatives. Why don't you write down the recipe of your chocolate
    cake and send it to them? Of course, the end result won't be ideal, especially
    if your friend does not cook well, but it will be much more efficient. A recipe
    is much lighter, it occupies much less space, and so it is faster to send.
</p>
    We can even have a third option. Let's say you meet your friend at a party and they
    ask you for your cake's recipe. You cannot remember it so you tell them that
    you'll check it and send it later. But because the recipe is very long, you decide
    to agree on some things that will make it shorter so that it is faster to send.
    You can agree, for example, that "One tablespoon" will be written as "1 Tbsp" (here
    you already save >50% of space), or that "1 milk" means "1 pack of oatmilk
    from <a href="https://www.oatly.com/en">Oatly</a>" (No, I don't receive any money from them &#128516;).
    You can save a bunch more space.
</p>
<p>
    That is the idea behind compression. The internet has "roads" that can be pretty congested.
    If you request a film from Netflix at 1080p, they can send the value (0 to 256)
    of each of the pixels (2073600) of each RGB channel (3) of each of the 130,000 frames of the film.
    That's roughly 809 GB of information (almost 1 TB). You can wait for your movie to
    download next year &#128516;...
    Of course, the less memory and time-efficient solution is also the one that has the best
    quality. So, if you were able to download all 809 GB, you can sure that it will be the
    best quality possible.
    But, we cannot wait for a year.
    The smaller the space that your information occupies, the faster it will be to send.
    But you still want the film to have a decent quality. 
    There is fundamental trade-off here called the
    <a href="https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory">Rate-Distortion curve</a>.
    We need to strike a balance between space occupied/speed and reconstruction quality. 
    If you wish to summarize your chocolate cake recipe in two lines, you can do it.
    But don't except the other person to come up with a version similar to your home-made cake.
</p>
    How can you do that in video? Well, videos are made of frames that oftentimes don't change
    much. A simple way to think about compression in this case is that if you have an almost static scene
    for 2 seconds, you can just encode the first frame and then send a small piece of information that
    says "The next 47 frames are the same".
    Otherwise, you can also encode one frame (called the intra-frame) and for the remaining
    ones in the next two seconds you compute the differences in motion (motion vectors)
    and just encode those.
    What we actually do is to encode this side information like motion vectors into
    a recipe themselves. So we have recipes for certain elements of the recipe!
</p>
<p>
    Netflix has an encoder, your laptop has a copy of a decoder.
    They are trained together, so that they can share information (like the third case
    in the chocolate cake example).
</p>
<p>
    In the second project we found a way to optimize a new recipe without
    having to exchange new information between the encoder and the decoder,
    just by tweaking the recipe itself.
    Knowing the information that I have and knowing the information my friend has,
    I can tweak the recipe in a way that I will either improve the quality or reduce
    the space, compared to a baseline scenario where I just generate the recipe (encode
    it) and don't tweak it.
</p>

    While there are many deep learning based approaches for single image compression, the field of end-to-end learned video coding has remained much less explored. Therefore, in this work we present an inter-frame compression approach for neural video coding that can seamlessly build up on different existing neural image codecs. Our end-to-end solution performs temporal prediction by optical flow based motion compensation in pixel space. The key insight is that we can increase both decoding efficiency and reconstruction quality by encoding the required side information into a latent representation that directly decodes into motion and blending coefficients. In order to account for remaining prediction errors, residual information between the original image and the interpolated frame is needed. We propose to compute residuals directly in latent space instead of in pixel space as this allows to reuse the same image compression network for both key frames and intermediate frames. This has the advantage of making our video coding approach, more coherent, more memory efficient, and easier to train. Our extended evaluation on different datasets and resolutions shows that the rate-distortion performance of our approach is competitive with existing state-of-the-art codecs.
    <br> <br>
    Conclusion: <br>
    Our neural video coding framework is able to achieve re-
    sults competitive with existing video codecs that have wit-
    nessed several decades of engineering improvements. This
    is in particular due to the interpolation approach that em-
    beds compression constraints and takes advantage of all
    the available information at encoding time. In addition to
    this, expressing residuals in latent space simplifies the video
    compression task as the same network is used both for key-
    frames and residuals. In this work, we have focused on
    compressing intermediate frames that rely on key frames in
    the past and future. However, our approach is also com-
    patible with other settings such as only frames from the
    past. Thus finding optimal strategies for key frame selec-
    tion would be an interesting branch of future work.
</p>
<p>
    Abstract: <br>
    The field of neural image compression has witnessed
    exciting progress as recently proposed architectures al-
    ready surpass the established transform coding based ap-
    proaches. While, so far, research has mainly focused on
    architecture and model improvements, in this work we ex-
    plore content adaptive optimization. To this end, we intro-
    duce an iterative procedure which adapts the latent repre-
    sentation to the specific content we wish to compress while
    keeping the parameters of the network and the predictive
    model fixed. Our experiments show that this allows for an
    overall increase in rate-distortion performance, indepen-
    dently of the specific architecture used. Furthermore, we
    also evaluate this strategy in the context of adapting a pre-
    trained network to other content that is different in visual
    appearance or resolution. Here, our experiments show that
    our adaptation strategy can largely close the gap as com-
    pared to models specifically trained for the given content
    while having the benefit that no additional data in the form
    of model parameter updates has to be transmitted.
    <br> <br>
    Conclusion: <br>
    In this work we have investigated content adaptive com-
    pression strategies which can be seen as a complementary
    approach of improving neural image coding besides archi-
    tecture refinements. More specifically, we have presented
    a latent space refinement algorithm that allows to improve
    quality by roughly 0.5 dB at the same bit rate on the Tecnick
    data set. This strategy also allows to significantly close the
    gap between generic pre-trained models and models that are
    specifically trained for a given target content.Thus, the la-
    tent space adaptation can be an effective strategy to make a
    given encoding process more powerful and content adap-
    tive. This is particularly beneficial in situations such as
    streaming, where the encoding complexity is not the limit-
    ing factor when compared to the transmission and decoding.
    As the gap towards models that are entirely trained on the
    specific target content cannot fully be closed, it would be
    interesting to further investigate which more complex but
    still practically viable form of adaptation may achieve this.
    Also currently, neural image compression models are typi-
    cally trained for each rate-distortion point and it would be
    similarly beneficial to investigate strategies that allow auto-
    matic adaptation to each quality level.
</p>
<h4> <b>Impact</b> </h4>
<p>
The work on video compression achieved state-of-the-art at the time.
It has been cited by 140 scientific publications, including publications of
<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_One-Shot_Free-View_Neural_Talking-Head_Synthesis_for_Video_Conferencing_CVPR_2021_paper.pdf">
Nvidia</a>,
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Agustsson_Scale-Space_Flow_for_End-to-End_Optimized_Video_Compression_CVPR_2020_paper.pdf">
Google</a>,
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9242247">Here</a>

<a href="https://proceedings.neurips.cc/paper_files/paper/2021/
file/96b250a90d3cf0868c83f8c965142d2a-Paper.pdf">
Microsoft</a>
</p>

Image Compression
This work has been cited over 30 times
<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9242247">Google</a>
<a href="https://arxiv.org/abs/2103.03123">University of Oxford</a>
<a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/
066f182b787111ed4cb65ed437f0855b-Paper.pdf">
</a>
University of California, Irvine (IMPORTANT)
<a href="https://openreview.net/pdf?id=IDwN6xjHnK8">
Qualcomm
</a>
<a href="https://arxiv.org/pdf/2008.09180.pdf">
Uber (CLOSE TO US)
</a>
<a href="https://openaccess.thecvf.com/content/CVPR2021W/CLIC/papers/Zhao_A_Universal_Encoder_Rate_Distortion_Optimization_Framework_for_Learned_Compression_CVPRW_2021_paper.pdf">
Microsoft (Important)
</a>
<a href="https://arxiv.org/pdf/2007.16054.pdf">Nokia</a>
<a href="https://arxiv.org/pdf/2108.09992.pdf">(Here too)</a>
<a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w7/Guo_Variable_Rate_Image_Compression_With_Content_Adaptive_Optimization_CVPRW_2020_paper.pdf">
Huawei
</a>

<div ng-controller="DisneyPubsCtrl">
    <pubs-no-headers></pubs-no-headers>
</div>
</div>  <!-- last element -->

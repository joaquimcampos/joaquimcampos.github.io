<h2 class="new-project">Learning with Hessian-Schatten Total Variation</h2>

<h3 style="margin: 0;">
    <a href="https://bigwww.epfl.ch/">@EPFL.BIG</a> |
    <a href="https://github.com/joaquimcampos/HTV-Learn">
        Github
        <github-logo></github-logo>
    </a>
</h3>

<a id="face-img" class="row-face" href="https://ieeexplore.ieee.org/document/9655475">
    <img src="assets/images/htv1.png" alt="Face GT">
    <img src="assets/images/htv2.png" alt="Face HTV">
    <img src="assets/images/htv3.png" alt="Face NN">
</a>

<div class="last-element">
<h4>This Project, Simply Explained</h4>
<p>
    Suppose we have a dataset \({\mathrm D}\) with \(M\) datapoints, where each datapoint is an
    input-value pair&mdash;mathematically, we write this statement
    as \({\mathrm D} = \lbrace{\mathbf x}_m, y_m\rbrace _{m=1}^M\).
    This dataset might be, for example, a sequence of \(M\) images of mountains and their
    corresponding height in meters, as depicted below. (We will use this example from now on.)
</p>
<div class="row-matterhorn">
    <img src="assets/images/matterhorn.png" alt="Radiobooks logo">
</div>
<p>
    In supervised learning our goal is to find a model \(f\) that predicts the height \(y_m\) of each image \({\mathbf x}_m\) in our dataset fairly well <a href="" ng-click="scrollToElement('footnote-1')">[1]</a>
    (<i>i.e.</i>, \(f({\bf x}_m) = \hat{y}_m \approx y_m\)),
    and that is able to generalize to new inputs. In other words, we are looking for a model that can accurately predict
    the heights of mountains from their respective images even when it has not
    seen those images before in the training data. (This is what learning is all about.)
</p>
<p>
    This is a very difficult task to solve in general, because there are infinite models
    that fit the data well enough. And, actually, very few actually make sense and are able
    to generalize.
    Therefore, we need to introduce some information to solve the problem.
<!-- </p>
<p> -->
    First, if we know something about the data distribution, then we can use
    that information. For example, if the data is the result of a linear mapping plus
    some observation noise, <i>i.e.</i>, \(y = {\mathbf w}^T{\mathbf x} + b + \epsilon\),
    then we can just search within family of models by just trying to find
    the optimal parameters \({\mathbf w}\) and \(b\); in this way, we transform an
    a priori very complicated (infinite-dimensional) problem into a very simple
    (finite-dimensional) one. Alternatively, we might restrict ourselves to a family
    of models that we know to contain models that perform well for similar tasks.
    Finally, we might also want to favor simple models, following the Occam's razor principle:
    as long as it can do the job, the simpler, the better.
    Simple models have the advantage of being faster to use,
    ocupying less memory and being more robust.
    Moreover, they are more explainable, since with less parameters the influence of
    each one in the final result becomes easier to understand,
    so that we actually have an idea of what is going on (unlike with ChatGPT &#x1F937).
    
</p>
<p>
    A family of models that has had a lot of success in the past few years
    are neural networks, which are piecewise-linear &#x1F4C9
    <a href="" ng-click="scrollToElement('footnote-2')">[2]</a>.
    The problem is that there is no way to control the number of linear regions of
    neural networks.
    In this project, we found a way to learn piecewise-linear models
    that have few regions and a straightforward relationship
    with their parameters.
    For this, we used a regularizer called Hessian-Schatten Total Variation (don't mind this)
    that enforces sparse second-derivatives (don't mind this either),
    and restricted ourselves to a family of models that is made up of "lego pieces" called box splines.
    In the following image you can see, on the left, a box spline and, on the right,
    an example of a learned model which results from putting different lego
    pieces with different heights in different locations (and summing them up).
</p>
<p> 
</p>
<div class="row-boxsplines">
    <div>
        <img src="assets/images/boxspline.png" alt="Box Spline">
    </div>
    <div></div>
    <div>
        <img src="assets/images/boxspline_model.png" alt="Piecewise-Linear Model">
    </div>
</div>
<p>
    The <a href="" ng-click="scrollToElement('face-img')">top image in this page</a>
    shows how our method compares to neural networks when trained on a dataset with
    \(M = 5000\) samples from a face.
    On the left you can see the ground-truth, in the middle our learned model,
    and on the right the neural network model. Surprisingly, neural network model
    does not seem to have many piecewise-linear regions.
    This might explain why neural nets are able to generalize well even when sometimes
    containing millions or billions (ChatGPT) of parameters.
</p>

<!-- <p>
    Abstract: <br>
    We develop a novel 2D functional learning framework that employs a sparsity-promoting
    regularization based on second-order derivatives. Motivated by the nature of the regularizer, we restrict the
    search space to the span of piecewise-linear box splines shifted on a 2D lattice. Our formulation of the
    infinite-dimensional problem on this search space allows us to recast it exactly as a finite-dimensional one
    that can be solved using standard methods in convex optimization. Since our search space is composed of
    continuous and piecewise-linear functions, our work presents itself as an alternative to training networks that
    deploy rectified linear units, which also construct models in this family. The advantages of our method are
    fourfold: the ability to enforce sparsity, favoring models with fewer piecewise-linear regions; the use of a rotation, scale and translation-invariant regularization; a single hyperparameter that controls the complexity of
    the model; and a clear model interpretability that provides a straightforward relation between the parameters
    and the overall learned function. We validate our framework in various experimental setups and compare it
    with neural networks.
    <br> <br>
    Conclusion: <br>
    We have introduced a method to solve two-dimensional learn-
    ing problems regularized with Hessian-nuclear total-variation
    (HTV) seminorm. The starting point of our work has been
    the observation that the HTV of (admissible) continuous and
    piecewise-linear (CPWL) functions has a closed-form expres-
    sion. Its computation, however, requires knowledge of the
    gradient and boundaries of each partition. To circumvent this
    drawback, we have formulated the problem in a search space
    consisting of shifts of CPWL box-splines in a lattice. By doing
    so, we are able to evaluate any model in the search space, as
    well as compute its HTV, from the values at the lattice points
    (model parameters). In particular, we showed that the latter
    can be computed with a three-filter convolutional structure;
    this allows us to discretize the problem exactly and to recast
    it in the form of the generalized least-absolute shrinkage-and-
    selection operator. Finally, we have demonstrated the sparsity-
    promoting effect of our framework via numerical examples
    where we have compared its performance with ReLU neural
    networks and radial-basis functions.
</p> -->
<h4 style="margin-top: 40px">Impact</h4>
<p>
This work has been cited 13 times. In particular, these results were used in two papers (<a href="https://arxiv.org/abs/2210.04077">[1]</a>,
<a href="https://arxiv.org/abs/2302.12554">[2]</a>) of mathematician
<a href="https://en.wikipedia.org/wiki/Luigi_Ambrosio">Luigi Ambrosio</a>,
leading expert in the calculus of variations and geometric measure theory, and
doctoral advisor of
<a href="https://en.wikipedia.org/wiki/Fields_Medal">Fields Medal</a>
winner
<a href="https://en.wikipedia.org/wiki/Alessio_Figalli">Alessio Figalli</a>.
</p>
<p style="margin-top: 20px; margin-bottom: 10px">
    <b>Publications</b>
</p>
<div ng-controller="HTVPubsCtrl">
    <pubs-no-headers></pubs-no-headers>
</div>

<h4 class="subsection">Technologies</h4>
<p>
    Here I outline a few technologies that I used in this project.
    <ul>
        <li><a href="https://www.python.org/">Python</a>: Programming language.</li>
        <li><a href="https://pytorch.org/">Pytorch</a>: Deep learning framework.</li>
        <li><a href="https://www.gnu.org/software/bash/">Bash</a>: Unix shell and command language.</li>
    </ul>
    
</p>

<hr style="margin-top: 40px;">
<h5>Footnotes</h5>
<p style="margin-bottom: 0px;" id="footnote-1">[1] We call this "fitting the data".</p>
<p id="footnote-2">[2] Those with ReLU-like activations, which are the most used ones.</p>

</div>  <!-- last element -->
